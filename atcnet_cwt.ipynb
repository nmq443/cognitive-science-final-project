{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmq443/cognitive-science-final-project/blob/main/atcnet_cwt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "SqN8BDu78pBn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqN8BDu78pBn",
        "outputId": "24d71c26-9e68-4dff-cc5c-1631cac854b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "D7J4mCDz-ZYJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7J4mCDz-ZYJ",
        "outputId": "7a9ccad8-fd57-408a-c3ce-4fb6a2ef6cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cognitive-science-final-project'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
            "remote: Total 282 (delta 55), reused 40 (delta 13), pack-reused 167\u001b[K\n",
            "Receiving objects: 100% (282/282), 5.75 MiB | 20.15 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nmq443/cognitive-science-final-project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "pWYGvzsx-mIl",
      "metadata": {
        "id": "pWYGvzsx-mIl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/content/cognitive-science-final-project/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "TQiT9MrS_NcJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQiT9MrS_NcJ",
        "outputId": "1783a4dc-33ab-4047-e674-a3d0c70f5226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/cognitive-science-final-project\n"
          ]
        }
      ],
      "source": [
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "21dabdfc-b1e4-42a8-bd1b-43aa389c2599",
      "metadata": {
        "id": "21dabdfc-b1e4-42a8-bd1b-43aa389c2599"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from preprocess import get_data\n",
        "from utils import getModel\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from evaluation import draw_learning_curves\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_model(dataset_conf, train_conf, results_path):\n",
        "\n",
        "    # remove the 'result' folder before training\n",
        "    if os.path.exists(results_path):\n",
        "        # Remove the folder and its contents\n",
        "        shutil.rmtree(results_path)\n",
        "        os.makedirs(results_path)\n",
        "\n",
        "    # Get the current 'IN' time to calculate the overall training time\n",
        "    in_exp = time.time()\n",
        "    # Create a file to store the path of the best model among several runs\n",
        "    best_models = open(results_path + \"/best models_cwt.txt\", \"w\")\n",
        "    # Create a file to store performance during training\n",
        "    log_write = open(results_path + \"/log.txt\", \"w\")\n",
        "\n",
        "    # Get dataset paramters\n",
        "    dataset = dataset_conf.get('name')\n",
        "    n_sub = dataset_conf.get('n_sub')\n",
        "    data_path = dataset_conf.get('data_path')\n",
        "    isStandard = dataset_conf.get('isStandard')\n",
        "    LOSO = dataset_conf.get('LOSO')\n",
        "    # Get training hyperparamters\n",
        "    batch_size = train_conf.get('batch_size')\n",
        "    epochs = train_conf.get('epochs')\n",
        "    patience = train_conf.get('patience')\n",
        "    lr = train_conf.get('lr')\n",
        "    LearnCurves = train_conf.get('LearnCurves') # Plot Learning Curves?\n",
        "    n_train = train_conf.get('n_train')\n",
        "    model_name = train_conf.get('model')\n",
        "    from_logits = train_conf.get('from_logits')\n",
        "\n",
        "    # Initialize variables\n",
        "    acc = np.zeros((n_sub, n_train))\n",
        "    kappa = np.zeros((n_sub, n_train))\n",
        "\n",
        "    # Iteration over subjects\n",
        "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "\n",
        "        print('\\nTraining on subject ', sub+1)\n",
        "        log_write.write( '\\nTraining on subject '+ str(sub+1) +'\\n')\n",
        "        # Initiating variables to save the best subject accuracy among multiple runs.\n",
        "        BestSubjAcc = 0\n",
        "        bestTrainingHistory = []\n",
        "\n",
        "        # Get training and test data\n",
        "        X_train, _, y_train_onehot, _, _, _ = get_data(\n",
        "            data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n",
        "\n",
        "        X_train = morlet_wavelet_transform(X_train)\n",
        "\n",
        "        # Divide the training data into training and validation\n",
        "        X_train, X_val, y_train_onehot, y_val_onehot = train_test_split(X_train, y_train_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Iteration over multiple runs\n",
        "        for train in range(n_train): # How many repetitions of training for subject i.\n",
        "            # Set the random seed for TensorFlow and NumPy random number generator.\n",
        "            # The purpose of setting a seed is to ensure reproducibility in random operations.\n",
        "            tf.random.set_seed(train+1)\n",
        "            np.random.seed(train+1)\n",
        "\n",
        "            # Get the current 'IN' time to calculate the 'run' training time\n",
        "            in_run = time.time()\n",
        "\n",
        "            # Create folders and files to save trained models for all runs\n",
        "            filepath = results_path + '/saved models_cwt/run-{}'.format(train+1)\n",
        "            if not os.path.exists(filepath):\n",
        "                os.makedirs(filepath)\n",
        "            filepath = filepath + '/subject-{}.h5'.format(sub+1)\n",
        "\n",
        "            # Create the model\n",
        "            model = getModel(model_name, dataset_conf, from_logits)\n",
        "            # Compile and train the model\n",
        "            model.compile(loss=CategoricalCrossentropy(from_logits=from_logits), optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "            # model.summary()\n",
        "            # plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "            callbacks = [\n",
        "                ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
        "                                save_best_only=True, save_weights_only=True, mode='min'),\n",
        "                ReduceLROnPlateau(monitor=\"val_loss\", factor=0.90, patience=20, verbose=0, min_lr=0.0001),\n",
        "                # EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\n",
        "            ]\n",
        "            history = model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot),\n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)\n",
        "\n",
        "            # Evaluate the performance of the trained model based on the validation data\n",
        "            # Here we load the Trained weights from the file saved in the hard\n",
        "            # disk, which should be the same as the weights of the current model.\n",
        "            model.load_weights(filepath)\n",
        "            y_pred = model.predict(X_val)\n",
        "\n",
        "            if from_logits:\n",
        "                y_pred = tf.nn.softmax(y_pred).numpy().argmax(axis=-1)\n",
        "            else:\n",
        "                y_pred = y_pred.argmax(axis=-1)\n",
        "\n",
        "            labels = y_val_onehot.argmax(axis=-1)\n",
        "            acc[sub, train]  = accuracy_score(labels, y_pred)\n",
        "            kappa[sub, train] = cohen_kappa_score(labels, y_pred)\n",
        "\n",
        "            # Get the current 'OUT' time to calculate the 'run' training time\n",
        "            out_run = time.time()\n",
        "            # Print & write performance measures for each run\n",
        "            info = 'Subject: {}   seed {}   time: {:.1f} m   '.format(sub+1, train+1, ((out_run-in_run)/60))\n",
        "            info = info + 'valid_acc: {:.4f}   valid_loss: {:.3f}'.format(acc[sub, train], min(history.history['val_loss']))\n",
        "            print(info)\n",
        "            log_write.write(info +'\\n')\n",
        "            # If current training run is better than previous runs, save the history.\n",
        "            if(BestSubjAcc < acc[sub, train]):\n",
        "                 BestSubjAcc = acc[sub, train]\n",
        "                 bestTrainingHistory = history\n",
        "\n",
        "        # Store the path of the best model among several runs\n",
        "        best_run = np.argmax(acc[sub,:])\n",
        "        filepath = '/saved models_cwt/run-{}/subject-{}.h5'.format(best_run+1, sub+1)+'\\n'\n",
        "        best_models.write(filepath)\n",
        "\n",
        "        # Plot Learning curves\n",
        "        if (LearnCurves == True):\n",
        "            print('Plot Learning Curves ....... ')\n",
        "            draw_learning_curves(bestTrainingHistory, sub+1)\n",
        "\n",
        "    # Get the current 'OUT' time to calculate the overall training time\n",
        "    out_exp = time.time()\n",
        "\n",
        "    # Print & write the validation performance using all seeds\n",
        "    head1 = head2 = '         '\n",
        "    for sub in range(n_sub):\n",
        "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
        "        head2 = head2 + '-----   '\n",
        "    head1 = head1 + '  average'\n",
        "    head2 = head2 + '  -------'\n",
        "    info = '\\n---------------------------------\\nValidation performance (acc %):'\n",
        "    info = info + '\\n---------------------------------\\n' + head1 +'\\n'+ head2\n",
        "    for run in range(n_train):\n",
        "        info = info + '\\nSeed {}:  '.format(run+1)\n",
        "        for sub in range(n_sub):\n",
        "            info = info + '{:.2f}   '.format(acc[sub, run]*100)\n",
        "        info = info + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
        "    info = info + '\\n---------------------------------\\nAverage acc - all seeds: '\n",
        "    info = info + '{:.2f} %\\n\\nTrain Time  - all seeds: {:.1f}'.format(np.average(acc)*100, (out_exp-in_exp)/(60))\n",
        "    info = info + ' min\\n---------------------------------\\n'\n",
        "    print(info)\n",
        "    log_write.write(info+'\\n')\n",
        "\n",
        "    # Close open files\n",
        "    best_models.close()\n",
        "    log_write.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "749100ae-592a-47f7-9bea-1171076b87b9",
      "metadata": {
        "id": "749100ae-592a-47f7-9bea-1171076b87b9"
      },
      "outputs": [],
      "source": [
        "def getModel(model_name, dataset_conf, from_logits = False):\n",
        "\n",
        "    n_classes = dataset_conf.get('n_classes')\n",
        "    n_channels = dataset_conf.get('n_channels')\n",
        "    in_samples = dataset_conf.get('in_samples')\n",
        "\n",
        "    # Select the model\n",
        "    if(model_name == 'ATCNet'):\n",
        "        # Train using the proposed ATCNet model: https://ieeexplore.ieee.org/document/9852687\n",
        "        model = models.ATCNet_(\n",
        "            # Dataset parameters\n",
        "            n_classes = n_classes,\n",
        "            in_chans = n_channels,\n",
        "            in_samples = in_samples,\n",
        "            # Sliding window (SW) parameter\n",
        "            n_windows = 5,\n",
        "            # Attention (AT) block parameter\n",
        "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
        "            # Convolutional (CV) block parameters\n",
        "            eegn_F1 = 16,\n",
        "            eegn_D = 2,\n",
        "            eegn_kernelSize = 64,\n",
        "            eegn_poolSize = 7,\n",
        "            eegn_dropout = 0.3,\n",
        "            # Temporal convolutional (TC) block parameters\n",
        "            tcn_depth = 2,\n",
        "            tcn_kernelSize = 4,\n",
        "            tcn_filters = 32,\n",
        "            tcn_dropout = 0.3,\n",
        "            tcn_activation='elu',\n",
        "            )\n",
        "    elif(model_name == 'ATCNet_CWT'):\n",
        "        model = models.ATCNet_CWT(\n",
        "            # Dataset parameters\n",
        "            n_classes = n_classes,\n",
        "            in_chans = n_channels,\n",
        "            in_samples = in_samples,\n",
        "            # Sliding window (SW) parameter\n",
        "            n_windows = 5,\n",
        "            # Attention (AT) block parameter\n",
        "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
        "            # Convolutional (CV) block parameters\n",
        "            eegn_F1 = 16,\n",
        "            eegn_D = 2,\n",
        "            eegn_kernelSize = 64,\n",
        "            eegn_poolSize = 7,\n",
        "            eegn_dropout = 0.3,\n",
        "            # Temporal convolutional (TC) block parameters\n",
        "            tcn_depth = 2,\n",
        "            tcn_kernelSize = 4,\n",
        "            tcn_filters = 32,\n",
        "            tcn_dropout = 0.3,\n",
        "            tcn_activation='elu',\n",
        "        )\n",
        "    elif(model_name == 'TCNet_Fusion'):\n",
        "        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n",
        "        model = models.TCNet_Fusion(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGTCNet'):\n",
        "        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n",
        "        model = models.EEGTCNet(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGNet'):\n",
        "        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n",
        "        model = models.EEGNet_classifier(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGNeX'):\n",
        "        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n",
        "        model = models.EEGNeX_8_32(n_timesteps = in_samples , n_features = n_channels, n_outputs = n_classes)\n",
        "    elif(model_name == 'DeepConvNet'):\n",
        "        # Train using DeepConvNet: https://doi.org/10.1002/hbm.23730\n",
        "        model = models.DeepConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "    elif(model_name == 'ShallowConvNet'):\n",
        "        # Train using ShallowConvNet: https://doi.org/10.1002/hbm.23730\n",
        "        model = models.ShallowConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "    elif(model_name == 'MBEEG_SENet'):\n",
        "        # Train using MBEEG_SENet: https://www.mdpi.com/2075-4418/12/4/995\n",
        "        model = models.MBEEG_SENet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"'{}' model is not supported yet!\".format(model_name))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e2c78ac7-2b27-43cc-98fa-076646a60a36",
      "metadata": {
        "id": "e2c78ac7-2b27-43cc-98fa-076646a60a36"
      },
      "outputs": [],
      "source": [
        "def test(model, dataset_conf, results_path, allRuns = True):\n",
        "    # Open the  \"Log\" file to write the evaluation results\n",
        "    log_write = open(results_path + \"/log.txt\", \"a\")\n",
        "\n",
        "    # Get dataset paramters\n",
        "    dataset = dataset_conf.get('name')\n",
        "    n_classes = dataset_conf.get('n_classes')\n",
        "    n_sub = dataset_conf.get('n_sub')\n",
        "    data_path = dataset_conf.get('data_path')\n",
        "    isStandard = dataset_conf.get('isStandard')\n",
        "    LOSO = dataset_conf.get('LOSO')\n",
        "    classes_labels = dataset_conf.get('cl_labels')\n",
        "\n",
        "    # Test the performance based on several runs (seeds)\n",
        "    runs = os.listdir(results_path+\"/saved models_cwt\")\n",
        "    # Initialize variables\n",
        "    acc = np.zeros((n_sub, len(runs)))\n",
        "    kappa = np.zeros((n_sub, len(runs)))\n",
        "    cf_matrix = np.zeros([n_sub, len(runs), n_classes, n_classes])\n",
        "\n",
        "    # Iteration over subjects\n",
        "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "    inference_time = 0 #  inference_time: classification time for one trial\n",
        "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "        # Load data\n",
        "        _, _, _, X_test, _, y_test_onehot = get_data(data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n",
        "\n",
        "        # Iteration over runs (seeds)\n",
        "        for seed in range(len(runs)):\n",
        "            # Load the model of the seed.\n",
        "            model.load_weights('{}/saved models_cwt/{}/subject-{}.h5'.format(results_path, runs[seed], sub+1))\n",
        "\n",
        "            inference_time = time.time()\n",
        "            # Predict MI task\n",
        "            y_pred = model.predict(X_test).argmax(axis=-1)\n",
        "            inference_time = (time.time() - inference_time)/X_test.shape[0]\n",
        "            # Calculate accuracy and K-score\n",
        "            labels = y_test_onehot.argmax(axis=-1)\n",
        "            acc[sub, seed]  = accuracy_score(labels, y_pred)\n",
        "            kappa[sub, seed] = cohen_kappa_score(labels, y_pred)\n",
        "            # Calculate and draw confusion matrix\n",
        "            cf_matrix[sub, seed, :, :] = confusion_matrix(labels, y_pred, normalize='true')\n",
        "            # draw_confusion_matrix(cf_matrix[sub, seed, :, :], str(sub+1), results_path, classes_labels)\n",
        "\n",
        "    # Print & write the average performance measures for all subjects\n",
        "    head1 = head2 = '                  '\n",
        "    for sub in range(n_sub):\n",
        "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
        "        head2 = head2 + '-----   '\n",
        "    head1 = head1 + '  average'\n",
        "    head2 = head2 + '  -------'\n",
        "    info = '\\n' + head1 +'\\n'+ head2\n",
        "    info = '\\n---------------------------------\\nTest performance (acc & k-score):\\n'\n",
        "    info = info + '---------------------------------\\n' + head1 +'\\n'+ head2\n",
        "    for run in range(len(runs)):\n",
        "        info = info + '\\nSeed {}: '.format(run+1)\n",
        "        info_acc = '(acc %)   '\n",
        "        info_k = '        (k-sco)   '\n",
        "        for sub in range(n_sub):\n",
        "            info_acc = info_acc + '{:.2f}   '.format(acc[sub, run]*100)\n",
        "            info_k = info_k + '{:.3f}   '.format(kappa[sub, run])\n",
        "        info_acc = info_acc + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
        "        info_k = info_k + '  {:.3f}   '.format(np.average(kappa[:, run]))\n",
        "        info = info + info_acc + '\\n' + info_k\n",
        "    info = info + '\\n----------------------------------\\nAverage - all seeds (acc %): '\n",
        "    info = info + '{:.2f}\\n                    (k-sco): '.format(np.average(acc)*100)\n",
        "    info = info + '{:.3f}\\n\\nInference time: {:.2f}'.format(np.average(kappa), inference_time * 1000)\n",
        "    info = info + ' ms per trial\\n----------------------------------\\n'\n",
        "    print(info)\n",
        "    log_write.write(info+'\\n')\n",
        "\n",
        "    # Draw a performance bar chart for all subjects\n",
        "    draw_performance_barChart(n_sub, acc.mean(1), 'Accuracy')\n",
        "    draw_performance_barChart(n_sub, kappa.mean(1), 'k-score')\n",
        "    # Draw confusion matrix for all subjects (average)\n",
        "    draw_confusion_matrix(cf_matrix.mean((0,1)), 'All', results_path, classes_labels)\n",
        "    # Close opened file\n",
        "    log_write.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "69b5e6d4-7029-4971-87fb-32e0c6a91caf",
      "metadata": {
        "id": "69b5e6d4-7029-4971-87fb-32e0c6a91caf"
      },
      "outputs": [],
      "source": [
        "# from train import train_model\n",
        "# from utils import getModel\n",
        "# from evaluation import test\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import numpy as np\n",
        "from preprocess import get_data\n",
        "import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4e8e9e11-c178-4c12-b46a-0c26d075d7af",
      "metadata": {
        "id": "4e8e9e11-c178-4c12-b46a-0c26d075d7af"
      },
      "outputs": [],
      "source": [
        "# if run on colab\n",
        "root_datapath = '/content/drive/MyDrive'\n",
        "\n",
        "# if run on local machine\n",
        "# root_datapath = '/home/quang'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5f467cdd-bfcd-41f3-92bb-2bae39218f2c",
      "metadata": {
        "id": "5f467cdd-bfcd-41f3-92bb-2bae39218f2c",
        "outputId": "1cd85f66-9a8e-4628-c5ca-b71a0f69aeca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/cognitive-science-final-project'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b7c6a147-544f-4c42-b964-e15e1bfcaf27",
      "metadata": {
        "id": "b7c6a147-544f-4c42-b964-e15e1bfcaf27"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "def morlet_wavelet_transform(X,fs=250,freq_range=(1,15),freq_bins=100,w=5):\n",
        "    '''\n",
        "    Discrete continous wavelet transform of eeg data convolved with complex morlet wavelet\n",
        "    INPUTS:\n",
        "    X - EEG data (num_trials, num_eeg_electrodes, time_bins,1)\n",
        "    fs - sampling rate in Hz\n",
        "    freq_range - tuple containing min and max freq range to perform analysis within\n",
        "    freq_bins - number of points between freq range being analyzed\n",
        "    w - Omega0 for complex morlet wavelet\n",
        "    OUTPUTS:\n",
        "    X_cwt - Wavlet transformed eeg data (num_trials, num_eeg_electrodes,freq_bins,time_bins)\n",
        "    '''\n",
        "\n",
        "    N_trials, _, N_eegs, time_bins = X.shape\n",
        "\n",
        "    # values for cwt\n",
        "    freq = np.linspace(freq_range[0],freq_range[1],freq_bins)\n",
        "    widths = w * fs / (2 * freq * np.pi)\n",
        "    X_cwt = np.zeros((N_trials, N_eegs, widths.shape[0], time_bins))\n",
        "\n",
        "    print('Performing discrete CWT convolutions...')\n",
        "    # for trial in tqdm_notebook(range(N_trials), desc='Trials'):\n",
        "    #     for eeg in tqdm_notebook(range(N_eegs), desc='EEG Channel', leave=False):\n",
        "    #         X_cwt[trial,eeg,:,:] = np.abs(signal.cwt(np.squeeze(X[trial,eeg,:,]),signal.morlet2,widths,w=w))\n",
        "\n",
        "    for trial in range(N_trials):\n",
        "        for eeg in range(N_eegs):\n",
        "            X_cwt[trial, eeg, :, :] = np.abs(signal.cwt(np.squeeze(X[trial, :, eeg, :]),signal.morlet2,widths,w=w))\n",
        "\n",
        "    return X_cwt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "105a3f77-4a7a-4eed-95de-83c0c78573d7",
      "metadata": {
        "id": "105a3f77-4a7a-4eed-95de-83c0c78573d7"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "    # Define dataset parameters\n",
        "    dataset = 'BCI2a' # Options: 'BCI2a','HGD', 'CS2R'\n",
        "\n",
        "    if dataset == 'BCI2a':\n",
        "        in_samples = 1125\n",
        "        n_channels = 22\n",
        "        n_sub = 9\n",
        "        n_classes = 4\n",
        "        classes_labels = ['Left hand', 'Right hand','Foot','Tongue']\n",
        "        data_path = os.path.expanduser(root_datapath) + '/BCI Competition IV/BCI Competition IV-2a/BCI Competition IV 2a mat/'\n",
        "    elif dataset == 'HGD':\n",
        "        in_samples = 1125\n",
        "        n_channels = 44\n",
        "        n_sub = 14\n",
        "        n_classes = 4\n",
        "        classes_labels = ['Right Hand', 'Left Hand','Rest','Feet']\n",
        "        data_path = os.path.expanduser(root_datapath) + '/mne_data/MNE-schirrmeister2017-data/robintibor/high-gamma-dataset/raw/master/data/'\n",
        "    elif dataset == 'CS2R':\n",
        "        in_samples = 1125\n",
        "        # in_samples = 576\n",
        "        n_channels = 32\n",
        "        n_sub = 18\n",
        "        n_classes = 3\n",
        "        # classes_labels = ['Fingers', 'Wrist','Elbow','Rest']\n",
        "        classes_labels = ['Fingers', 'Wrist','Elbow']\n",
        "        # classes_labels = ['Fingers', 'Elbow']\n",
        "        data_path = os.path.expanduser(root_datapath) + '/CS2R MI EEG dataset/all/EDF - Cleaned - phase one (remove extra runs)/two sessions/'\n",
        "    else:\n",
        "        raise Exception(\"'{}' dataset is not supported yet!\".format(dataset))\n",
        "\n",
        "    # Create a folder to store the results of the experiment\n",
        "    results_path = os.getcwd() + \"/results\"\n",
        "    if not  os.path.exists(results_path):\n",
        "      os.makedirs(results_path)   # Create a new directory if it does not exist\n",
        "\n",
        "    # Set dataset paramters\n",
        "    dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n",
        "                    'n_sub': n_sub, 'n_channels': n_channels, 'in_samples': in_samples,\n",
        "                    'data_path': data_path, 'isStandard': True, 'LOSO': False}\n",
        "    # Set training hyperparamters\n",
        "    train_conf = { 'batch_size': 64, 'epochs': 1, 'patience': 100, 'lr': 0.001,'n_train': 1,\n",
        "                  'LearnCurves': True, 'from_logits': False, 'model':'ATCNet_CWT'}\n",
        "\n",
        "    # Train the model\n",
        "    train_model(dataset_conf, train_conf, results_path)\n",
        "\n",
        "    # Evaluate the model based on the weights saved in the '/results' folder\n",
        "    model = getModel(train_conf.get('model'), dataset_conf)\n",
        "    test(model, dataset_conf, results_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bef05f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bef05f1",
        "outputId": "d758032c-ce92-4e9f-b6b0-4504a3941c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on subject  1\n",
            "Performing discrete CWT convolutions...\n"
          ]
        }
      ],
      "source": [
        "run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e020b5",
      "metadata": {
        "id": "74e020b5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}