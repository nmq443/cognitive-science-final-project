{"metadata":{"colab":{"provenance":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8455971,"sourceType":"datasetVersion","datasetId":5039749}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/nmq443/cognitive-science-final-project/blob/main/atcnet_cwt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"# if run on colab\n# from google.colab import drive\n# drive.mount('/content/drive',force_remount=True)","metadata":{"id":"SqN8BDu78pBn","outputId":"32a46787-39b5-47d6-b161-727fb7271f60","execution":{"iopub.status.busy":"2024-05-23T05:29:58.462495Z","iopub.status.idle":"2024-05-23T05:29:58.463041Z","shell.execute_reply.started":"2024-05-23T05:29:58.462689Z","shell.execute_reply":"2024-05-23T05:29:58.462705Z"},"colab":{"base_uri":"https://localhost:8080/"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"code","source":"!pip show tensorflow","metadata":{"id":"4gFjQCn-xWj5","outputId":"ad3e89d8-98b8-475b-8c22-2e448a56edb7","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-05-27T02:06:14.563347Z","iopub.execute_input":"2024-05-27T02:06:14.563729Z","iopub.status.idle":"2024-05-27T02:06:30.986425Z","shell.execute_reply.started":"2024-05-27T02:06:14.563700Z","shell.execute_reply":"2024-05-27T02:06:30.985164Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Name: tensorflow\nVersion: 2.15.0\nSummary: TensorFlow is an open source machine learning framework for everyone.\nHome-page: https://www.tensorflow.org/\nAuthor: Google Inc.\nAuthor-email: packages@tensorflow.org\nLicense: Apache 2.0\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\nRequired-by: explainable-ai-sdk, tensorflow-cloud, tensorflow-decision-forests, tensorflow-serving-api, tensorflow-text, tf_keras, witwidget\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/nmq443/cognitive-science-final-project.git","metadata":{"id":"D7J4mCDz-ZYJ","outputId":"d8679eeb-dba4-4935-dfef-81edb7a23d04","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-05-27T02:06:48.792931Z","iopub.execute_input":"2024-05-27T02:06:48.794589Z","iopub.status.idle":"2024-05-27T02:06:51.058636Z","shell.execute_reply.started":"2024-05-27T02:06:48.794526Z","shell.execute_reply":"2024-05-27T02:06:51.057546Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'cognitive-science-final-project'...\nremote: Enumerating objects: 297, done.\u001b[K\nremote: Counting objects: 100% (130/130), done.\u001b[K\nremote: Compressing objects: 100% (117/117), done.\u001b[K\nremote: Total 297 (delta 63), reused 39 (delta 13), pack-reused 167\u001b[K\nReceiving objects: 100% (297/297), 5.87 MiB | 29.03 MiB/s, done.\nResolving deltas: 100% (143/143), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# if run on colab\n# os.chdir('/content/cognitive-science-final-project/')\n\n# if run on kaggle\nos.chdir('cognitive-science-final-project/')\nos.getcwd()","metadata":{"id":"pWYGvzsx-mIl","outputId":"68316683-f11c-4f56-8f24-150b57dd405b","colab":{"base_uri":"https://localhost:8080/","height":35},"execution":{"iopub.status.busy":"2024-05-27T02:06:55.529975Z","iopub.execute_input":"2024-05-27T02:06:55.531342Z","iopub.status.idle":"2024-05-27T02:06:55.543422Z","shell.execute_reply.started":"2024-05-27T02:06:55.531292Z","shell.execute_reply":"2024-05-27T02:06:55.541994Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/cognitive-science-final-project'"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport time\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import cohen_kappa_score\nfrom preprocess import get_data\nfrom utils import getModel\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom evaluation import draw_learning_curves\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score\n\ndef train_model(dataset_conf, train_conf, results_path, cwt=True):\n\n    # remove the 'result' folder before training\n    if os.path.exists(results_path):\n        # Remove the folder and its contents\n        shutil.rmtree(results_path)\n        os.makedirs(results_path)\n\n    # Get the current 'IN' time to calculate the overall training time\n    in_exp = time.time()\n    # Create a file to store the path of the best model among several runs\n    best_models = open(results_path + \"/best models_cwt.txt\", \"w\")\n    # Create a file to store performance during training\n    log_write = open(results_path + \"/log.txt\", \"w\")\n\n    # Get dataset paramters\n    dataset = dataset_conf.get('name')\n    n_sub = dataset_conf.get('n_sub')\n    data_path = dataset_conf.get('data_path')\n    isStandard = dataset_conf.get('isStandard')\n    LOSO = dataset_conf.get('LOSO')\n    # Get training hyperparamters\n    batch_size = train_conf.get('batch_size')\n    epochs = train_conf.get('epochs')\n    patience = train_conf.get('patience')\n    lr = train_conf.get('lr')\n    LearnCurves = train_conf.get('LearnCurves') # Plot Learning Curves?\n    n_train = train_conf.get('n_train')\n    model_name = train_conf.get('model')\n    from_logits = train_conf.get('from_logits')\n\n    # Initialize variables\n    acc = np.zeros((n_sub, n_train))\n    kappa = np.zeros((n_sub, n_train))\n\n    # Iteration over subjects\n    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n\n        print('\\nTraining on subject ', sub+1)\n        log_write.write( '\\nTraining on subject '+ str(sub+1) +'\\n')\n        # Initiating variables to save the best subject accuracy among multiple runs.\n        BestSubjAcc = 0\n        bestTrainingHistory = []\n\n        # Get training and test data\n        X_train, _, y_train_onehot, _, _, _ = get_data(\n            data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n\n        if cwt:\n            X_train = morlet_wavelet_transform(X_train)\n\n        # Divide the training data into training and validation\n        X_train, X_val, y_train_onehot, y_val_onehot = train_test_split(X_train, y_train_onehot, test_size=0.2, random_state=42)\n\n        # Iteration over multiple runs\n        for train in range(n_train): # How many repetitions of training for subject i.\n            # Set the random seed for TensorFlow and NumPy random number generator.\n            # The purpose of setting a seed is to ensure reproducibility in random operations.\n            tf.random.set_seed(train+1)\n            np.random.seed(train+1)\n\n            # Get the current 'IN' time to calculate the 'run' training time\n            in_run = time.time()\n\n            # Create folders and files to save trained models for all runs\n            filepath = results_path + '/saved models_cwt/run-{}'.format(train+1)\n            if not os.path.exists(filepath):\n                os.makedirs(filepath)\n            # filepath = filepath + '/subject-{}.weights.h5'.format(sub+1)\n            filepath = filepath + '/subject-{}.h5'.format(sub+1)\n\n            # Create the model\n            model = getModel(model_name, dataset_conf, from_logits)\n            # Compile and train the model\n            model.compile(loss=CategoricalCrossentropy(from_logits=from_logits), optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n\n            # model.summary()\n            # plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\n\n            callbacks = [\n                ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n                                save_best_only=True, save_weights_only=True, mode='min'),\n                ReduceLROnPlateau(monitor=\"val_loss\", factor=0.90, patience=20, verbose=0, min_lr=0.0001),\n                # EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\n            ]\n\n            history = model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot),\n                                epochs=epochs, batch_size=batch_size, verbose=0)\n\n            # Evaluate the performance of the trained model based on the validation data\n            # Here we load the Trained weights from the file saved in the hard\n            # disk, which should be the same as the weights of the current model.\n#             model.load_weights(filepath)\n            y_pred = model.predict(X_val)\n\n            if from_logits:\n                y_pred = tf.nn.softmax(y_pred).numpy().argmax(axis=-1)\n            else:\n                y_pred = y_pred.argmax(axis=-1)\n\n            labels = y_val_onehot.argmax(axis=-1)\n            acc[sub, train]  = accuracy_score(labels, y_pred)\n            kappa[sub, train] = cohen_kappa_score(labels, y_pred)\n\n            # Get the current 'OUT' time to calculate the 'run' training time\n            out_run = time.time()\n            # Print & write performance measures for each run\n            info = 'Subject: {}   seed {}   time: {:.1f} m   '.format(sub+1, train+1, ((out_run-in_run)/60))\n            info = info + 'valid_acc: {:.4f}   valid_loss: {:.3f}'.format(acc[sub, train], min(history.history['val_loss']))\n            print(info)\n            log_write.write(info +'\\n')\n            # If current training run is better than previous runs, save the history.\n            if(BestSubjAcc < acc[sub, train]):\n                 BestSubjAcc = acc[sub, train]\n                 bestTrainingHistory = history\n\n        # Store the path of the best model among several runs\n        best_run = np.argmax(acc[sub,:])\n        # filepath = '/saved models cwt/run-{}/subject-{}.weights.h5'.format(best_run+1, sub+1)+'\\n'\n        filepath = '/saved models cwt/run-{}/subject-{}.h5'.format(best_run+1, sub+1)+'\\n'\n        best_models.write(filepath)\n\n        # Plot Learning curves\n        if (LearnCurves == True):\n            print('Plot Learning Curves ....... ')\n            draw_learning_curves(bestTrainingHistory, sub+1)\n\n    # Get the current 'OUT' time to calculate the overall training time\n    out_exp = time.time()\n\n    # Print & write the validation performance using all seeds\n    head1 = head2 = '         '\n    for sub in range(n_sub):\n        head1 = head1 + 'sub_{}   '.format(sub+1)\n        head2 = head2 + '-----   '\n    head1 = head1 + '  average'\n    head2 = head2 + '  -------'\n    info = '\\n---------------------------------\\nValidation performance (acc %):'\n    info = info + '\\n---------------------------------\\n' + head1 +'\\n'+ head2\n    for run in range(n_train):\n        info = info + '\\nSeed {}:  '.format(run+1)\n        for sub in range(n_sub):\n            info = info + '{:.2f}   '.format(acc[sub, run]*100)\n        info = info + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n    info = info + '\\n---------------------------------\\nAverage acc - all seeds: '\n    info = info + '{:.2f} %\\n\\nTrain Time  - all seeds: {:.1f}'.format(np.average(acc)*100, (out_exp-in_exp)/(60))\n    info = info + ' min\\n---------------------------------\\n'\n    print(info)\n    log_write.write(info+'\\n')\n\n    # Close open files\n    best_models.close()\n    log_write.close()","metadata":{"id":"21dabdfc-b1e4-42a8-bd1b-43aa389c2599","execution":{"iopub.status.busy":"2024-05-27T02:07:04.114364Z","iopub.execute_input":"2024-05-27T02:07:04.114744Z","iopub.status.idle":"2024-05-27T02:07:20.305728Z","shell.execute_reply.started":"2024-05-27T02:07:04.114715Z","shell.execute_reply":"2024-05-27T02:07:20.304620Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-27 02:07:07.198383: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-27 02:07:07.198522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-27 02:07:07.369897: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def getModel(model_name, dataset_conf, from_logits = False):\n\n    n_classes = dataset_conf.get('n_classes')\n    n_channels = dataset_conf.get('n_channels')\n    in_samples = dataset_conf.get('in_samples')\n\n    # Select the model\n    if(model_name == 'ATCNet'):\n        # Train using the proposed ATCNet model: https://ieeexplore.ieee.org/document/9852687\n        model = models.ATCNet_(\n            # Dataset parameters\n            n_classes = n_classes,\n            in_chans = n_channels,\n            in_samples = in_samples,\n            # Sliding window (SW) parameter\n            n_windows = 5,\n            # Attention (AT) block parameter\n            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n            # Convolutional (CV) block parameters\n            eegn_F1 = 16,\n            eegn_D = 2,\n            eegn_kernelSize = 64,\n            eegn_poolSize = 7,\n            eegn_dropout = 0.3,\n            # Temporal convolutional (TC) block parameters\n            tcn_depth = 2,\n            tcn_kernelSize = 4,\n            tcn_filters = 32,\n            tcn_dropout = 0.3,\n            tcn_activation='elu',\n            )\n    elif(model_name == 'ATCNet_CWT'):\n        model = models.ATCNet_CWT(\n            # Dataset parameters\n            n_classes = n_classes,\n            in_chans = n_channels,\n            in_samples = in_samples,\n            # Sliding window (SW) parameter\n            n_windows = 5,\n            # Attention (AT) block parameter\n            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n            # Convolutional (CV) block parameters\n            eegn_F1 = 16,\n            eegn_D = 2,\n            eegn_kernelSize = 64,\n            eegn_poolSize = 7,\n            eegn_dropout = 0.3,\n            # Temporal convolutional (TC) block parameters\n            tcn_depth = 2,\n            tcn_kernelSize = 4,\n            tcn_filters = 32,\n            tcn_dropout = 0.3,\n            tcn_activation='elu',\n        )\n    elif(model_name == 'TCNet_Fusion'):\n        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n        model = models.TCNet_Fusion(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n    elif(model_name == 'EEGTCNet'):\n        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n        model = models.EEGTCNet(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n    elif(model_name == 'EEGNet'):\n        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n        model = models.EEGNet_classifier(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n    elif(model_name == 'EEGNeX'):\n        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n        model = models.EEGNeX_8_32(n_timesteps = in_samples , n_features = n_channels, n_outputs = n_classes)\n    elif(model_name == 'DeepConvNet'):\n        # Train using DeepConvNet: https://doi.org/10.1002/hbm.23730\n        model = models.DeepConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n    elif(model_name == 'ShallowConvNet'):\n        # Train using ShallowConvNet: https://doi.org/10.1002/hbm.23730\n        model = models.ShallowConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n    elif(model_name == 'MBEEG_SENet'):\n        # Train using MBEEG_SENet: https://www.mdpi.com/2075-4418/12/4/995\n        model = models.MBEEG_SENet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n\n    else:\n        raise Exception(\"'{}' model is not supported yet!\".format(model_name))\n\n    return model","metadata":{"id":"749100ae-592a-47f7-9bea-1171076b87b9","execution":{"iopub.status.busy":"2024-05-27T02:07:42.078363Z","iopub.execute_input":"2024-05-27T02:07:42.078835Z","iopub.status.idle":"2024-05-27T02:07:42.094947Z","shell.execute_reply.started":"2024-05-27T02:07:42.078802Z","shell.execute_reply":"2024-05-27T02:07:42.093716Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def test(model, dataset_conf, results_path, allRuns = True):\n    # Open the  \"Log\" file to write the evaluation results\n    log_write = open(results_path + \"/log.txt\", \"a\")\n\n    # Get dataset paramters\n    dataset = dataset_conf.get('name')\n    n_classes = dataset_conf.get('n_classes')\n    n_sub = dataset_conf.get('n_sub')\n    data_path = dataset_conf.get('data_path')\n    isStandard = dataset_conf.get('isStandard')\n    LOSO = dataset_conf.get('LOSO')\n    classes_labels = dataset_conf.get('cl_labels')\n\n    # Test the performance based on several runs (seeds)\n    runs = os.listdir(results_path+\"/saved models cwt\")\n    # Initialize variables\n    acc = np.zeros((n_sub, len(runs)))\n    kappa = np.zeros((n_sub, len(runs)))\n    cf_matrix = np.zeros([n_sub, len(runs), n_classes, n_classes])\n\n    # Iteration over subjects\n    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n    inference_time = 0 #  inference_time: classification time for one trial\n    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n        # Load data\n        _, _, _, X_test, _, y_test_onehot = get_data(data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n\n        # Iteration over runs (seeds)\n        for seed in range(len(runs)):\n            # Load the model of the seed.\n            # model.load_weights('{}/saved models cwt/{}/subject-{}.weights.h5'.format(results_path, runs[seed], sub+1))\n            model.load_weights('{}/saved models cwt/{}/subject-{}.h5'.format(results_path, runs[seed], sub+1))\n\n\n            inference_time = time.time()\n            # Predict MI task\n            y_pred = model.predict(X_test).argmax(axis=-1)\n            inference_time = (time.time() - inference_time)/X_test.shape[0]\n            # Calculate accuracy and K-score\n            labels = y_test_onehot.argmax(axis=-1)\n            acc[sub, seed]  = accuracy_score(labels, y_pred)\n            kappa[sub, seed] = cohen_kappa_score(labels, y_pred)\n            # Calculate and draw confusion matrix\n            cf_matrix[sub, seed, :, :] = confusion_matrix(labels, y_pred, normalize='true')\n            # draw_confusion_matrix(cf_matrix[sub, seed, :, :], str(sub+1), results_path, classes_labels)\n\n    # Print & write the average performance measures for all subjects\n    head1 = head2 = '                  '\n    for sub in range(n_sub):\n        head1 = head1 + 'sub_{}   '.format(sub+1)\n        head2 = head2 + '-----   '\n    head1 = head1 + '  average'\n    head2 = head2 + '  -------'\n    info = '\\n' + head1 +'\\n'+ head2\n    info = '\\n---------------------------------\\nTest performance (acc & k-score):\\n'\n    info = info + '---------------------------------\\n' + head1 +'\\n'+ head2\n    for run in range(len(runs)):\n        info = info + '\\nSeed {}: '.format(run+1)\n        info_acc = '(acc %)   '\n        info_k = '        (k-sco)   '\n        for sub in range(n_sub):\n            info_acc = info_acc + '{:.2f}   '.format(acc[sub, run]*100)\n            info_k = info_k + '{:.3f}   '.format(kappa[sub, run])\n        info_acc = info_acc + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n        info_k = info_k + '  {:.3f}   '.format(np.average(kappa[:, run]))\n        info = info + info_acc + '\\n' + info_k\n    info = info + '\\n----------------------------------\\nAverage - all seeds (acc %): '\n    info = info + '{:.2f}\\n                    (k-sco): '.format(np.average(acc)*100)\n    info = info + '{:.3f}\\n\\nInference time: {:.2f}'.format(np.average(kappa), inference_time * 1000)\n    info = info + ' ms per trial\\n----------------------------------\\n'\n    print(info)\n    log_write.write(info+'\\n')\n\n    # Draw a performance bar chart for all subjects\n    draw_performance_barChart(n_sub, acc.mean(1), 'Accuracy')\n    draw_performance_barChart(n_sub, kappa.mean(1), 'k-score')\n    # Draw confusion matrix for all subjects (average)\n    draw_confusion_matrix(cf_matrix.mean((0,1)), 'All', results_path, classes_labels)\n    # Close opened file\n    log_write.close()","metadata":{"id":"e2c78ac7-2b27-43cc-98fa-076646a60a36","execution":{"iopub.status.busy":"2024-05-27T02:07:42.160913Z","iopub.execute_input":"2024-05-27T02:07:42.161345Z","iopub.status.idle":"2024-05-27T02:07:42.184485Z","shell.execute_reply.started":"2024-05-27T02:07:42.161311Z","shell.execute_reply":"2024-05-27T02:07:42.183286Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# from train import train_model\n# from utils import getModel\n# from evaluation import test\nimport os\nimport shutil\nimport time\nimport numpy as np\nfrom preprocess import get_data\nimport models","metadata":{"id":"69b5e6d4-7029-4971-87fb-32e0c6a91caf","execution":{"iopub.status.busy":"2024-05-27T02:07:43.035328Z","iopub.execute_input":"2024-05-27T02:07:43.035726Z","iopub.status.idle":"2024-05-27T02:07:43.041800Z","shell.execute_reply.started":"2024-05-27T02:07:43.035697Z","shell.execute_reply":"2024-05-27T02:07:43.040401Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# if run on colab\n# root_datapath = '/content/drive/MyDrive'\n\n# if run on local machine\n# root_datapath = '/home/quang'\n\n# if run on kaggle\nroot_datapath = '/kaggle/input/bci-competition-iv-2a'","metadata":{"id":"4e8e9e11-c178-4c12-b46a-0c26d075d7af","execution":{"iopub.status.busy":"2024-05-27T02:07:48.387578Z","iopub.execute_input":"2024-05-27T02:07:48.388277Z","iopub.status.idle":"2024-05-27T02:07:48.392714Z","shell.execute_reply.started":"2024-05-27T02:07:48.388237Z","shell.execute_reply":"2024-05-27T02:07:48.391870Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import signal\nimport matplotlib.pyplot as plt\n\n\ndef morlet_wavelet_transform(X,fs=250,freq_range=(1,15),freq_bins=100,w=5):\n    '''\n    Discrete continous wavelet transform of eeg data convolved with complex morlet wavelet\n    INPUTS:\n    X - EEG data (num_trials, num_eeg_electrodes, time_bins,1)\n    fs - sampling rate in Hz\n    freq_range - tuple containing min and max freq range to perform analysis within\n    freq_bins - number of points between freq range being analyzed\n    w - Omega0 for complex morlet wavelet\n    OUTPUTS:\n    X_cwt - Wavlet transformed eeg data (num_trials, num_eeg_electrodes,freq_bins,time_bins)\n    '''\n\n    N_trials, _, N_eegs, time_bins = X.shape\n\n    # values for cwt\n    freq = np.linspace(freq_range[0],freq_range[1],freq_bins)\n    widths = w * fs / (2 * freq * np.pi)\n    X_cwt = np.zeros((N_trials, N_eegs, widths.shape[0], time_bins))\n\n    print('Performing discrete CWT convolutions...')\n\n    for trial in range(N_trials):\n        for eeg in range(N_eegs):\n            X_cwt[trial, eeg, :, :] = np.abs(signal.cwt(np.squeeze(X[trial, :, eeg, :]),signal.morlet2,widths,w=w))\n\n    return X_cwt","metadata":{"id":"b7c6a147-544f-4c42-b964-e15e1bfcaf27","execution":{"iopub.status.busy":"2024-05-27T02:07:50.462526Z","iopub.execute_input":"2024-05-27T02:07:50.462903Z","iopub.status.idle":"2024-05-27T02:07:50.634832Z","shell.execute_reply.started":"2024-05-27T02:07:50.462867Z","shell.execute_reply":"2024-05-27T02:07:50.633897Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"os.getcwd()","metadata":{"id":"L4qidb2jv1CH","outputId":"b28b6054-3ec9-4bb4-dce7-1c4791b1e5c2","colab":{"base_uri":"https://localhost:8080/","height":35},"execution":{"iopub.status.busy":"2024-05-27T02:07:52.644613Z","iopub.execute_input":"2024-05-27T02:07:52.645041Z","iopub.status.idle":"2024-05-27T02:07:52.652417Z","shell.execute_reply.started":"2024-05-27T02:07:52.645007Z","shell.execute_reply":"2024-05-27T02:07:52.650813Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/cognitive-science-final-project'"},"metadata":{}}]},{"cell_type":"code","source":"def run():\n    # Define dataset parameters\n    dataset = 'BCI2a' # Options: 'BCI2a','HGD', 'CS2R'\n\n    if dataset == 'BCI2a':\n        in_samples = 1125\n        n_channels = 22\n        n_sub = 9\n        n_classes = 4\n        classes_labels = ['Left hand', 'Right hand','Foot','Tongue']\n        data_path = os.path.expanduser(root_datapath) + '/BCI Competition IV/BCI Competition IV-2a/BCI Competition IV 2a mat/'\n    elif dataset == 'HGD':\n        in_samples = 1125\n        n_channels = 44\n        n_sub = 14\n        n_classes = 4\n        classes_labels = ['Right Hand', 'Left Hand','Rest','Feet']\n        data_path = os.path.expanduser(root_datapath) + '/mne_data/MNE-schirrmeister2017-data/robintibor/high-gamma-dataset/raw/master/data/'\n    elif dataset == 'CS2R':\n        in_samples = 1125\n        # in_samples = 576\n        n_channels = 32\n        n_sub = 18\n        n_classes = 3\n        # classes_labels = ['Fingers', 'Wrist','Elbow','Rest']\n        classes_labels = ['Fingers', 'Wrist','Elbow']\n        # classes_labels = ['Fingers', 'Elbow']\n        data_path = os.path.expanduser(root_datapath) + '/CS2R MI EEG dataset/all/EDF - Cleaned - phase one (remove extra runs)/two sessions/'\n    else:\n        raise Exception(\"'{}' dataset is not supported yet!\".format(dataset))\n\n    # Create a folder to store the results of the experiment\n    results_path = os.getcwd() + \"/results\"\n    if not os.path.exists(results_path):\n      os.makedirs(results_path)   # Create a new directory if it does not exist\n\n    # Set dataset paramters\n    dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n                    'n_sub': n_sub, 'n_channels': n_channels, 'in_samples': in_samples,\n                    'data_path': data_path, 'isStandard': True, 'LOSO': False}\n    # Set training hyperparamters\n    train_conf = { 'batch_size': 64, 'epochs': 50, 'patience': 100, 'lr': 0.001,'n_train': 1,\n                  'LearnCurves': True, 'from_logits': False, 'model':'ATCNet'}\n\n    # Train the model\n    train_model(dataset_conf, train_conf, results_path, cwt=False)\n\n    # Evaluate the model based on the weights saved in the '/results' folder\n    model = getModel(train_conf.get('model'), dataset_conf)\n    test(model, dataset_conf, results_path)","metadata":{"id":"105a3f77-4a7a-4eed-95de-83c0c78573d7","execution":{"iopub.status.busy":"2024-05-27T02:07:56.066258Z","iopub.execute_input":"2024-05-27T02:07:56.066677Z","iopub.status.idle":"2024-05-27T02:07:56.080294Z","shell.execute_reply.started":"2024-05-27T02:07:56.066646Z","shell.execute_reply":"2024-05-27T02:07:56.079105Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip uninstall -y keras\n!pip install keras==2.9.0","metadata":{"execution":{"iopub.status.busy":"2024-05-27T02:10:55.493665Z","iopub.execute_input":"2024-05-27T02:10:55.494275Z","iopub.status.idle":"2024-05-27T02:11:14.206285Z","shell.execute_reply.started":"2024-05-27T02:10:55.494226Z","shell.execute_reply":"2024-05-27T02:11:14.204690Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Found existing installation: keras 2.9.0\nUninstalling keras-2.9.0:\n  Successfully uninstalled keras-2.9.0\nCollecting keras==2.9.0\n  Using cached keras-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\nUsing cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\nInstalling collected packages: keras\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.9.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip uninstall -y tensorflow\n!pip install tensorflow==2.9.1","metadata":{"execution":{"iopub.status.busy":"2024-05-27T02:13:51.329225Z","iopub.execute_input":"2024-05-27T02:13:51.329620Z","iopub.status.idle":"2024-05-27T02:15:35.022629Z","shell.execute_reply.started":"2024-05-27T02:13:51.329592Z","shell.execute_reply":"2024-05-27T02:15:35.020289Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Found existing installation: tensorflow 2.15.0\nUninstalling tensorflow-2.15.0:\n  Successfully uninstalled tensorflow-2.15.0\nCollecting tensorflow==2.9.1\n  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.6.3)\nCollecting flatbuffers<2,>=1.12 (from tensorflow==2.9.1)\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\nCollecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.1)\n  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.60.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (3.10.0)\nRequirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (2.9.0)\nCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.1)\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (16.0.6)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (21.3)\nCollecting protobuf<3.20,>=3.9.2 (from tensorflow==2.9.1)\n  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.16.0)\nCollecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.1)\n  Downloading tensorboard-2.9.1-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (0.35.0)\nCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.1)\n  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (4.9.0)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9.1) (1.14.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.26.1)\nCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.31.0)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\nCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.1)\n  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.9.1) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.2)\nDownloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, gast, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 23.5.26\n    Uninstalling flatbuffers-23.5.26:\n      Successfully uninstalled flatbuffers-23.5.26\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.15.0\n    Uninstalling tensorflow-estimator-2.15.0:\n      Successfully uninstalled tensorflow-estimator-2.15.0\n  Attempting uninstall: tensorboard-data-server\n    Found existing installation: tensorboard-data-server 0.7.2\n    Uninstalling tensorboard-data-server-0.7.2:\n      Successfully uninstalled tensorboard-data-server-0.7.2\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: gast\n    Found existing installation: gast 0.5.4\n    Uninstalling gast-0.5.4:\n      Successfully uninstalled gast-0.5.4\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.2.0\n    Uninstalling google-auth-oauthlib-1.2.0:\n      Successfully uninstalled google-auth-oauthlib-1.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nonnx 1.16.0 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\ntensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.9.1 which is incompatible.\ntensorflow-serving-api 2.14.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-serving-api 2.14.1 requires tensorflow<3,>=2.14.1, but you have tensorflow 2.9.1 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.1 which is incompatible.\ntf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n","output_type":"stream"}]},{"cell_type":"code","source":"run()","metadata":{"id":"6bef05f1","outputId":"c22a459f-e696-4dd5-b6ee-9d3d8e3e0aff","colab":{"base_uri":"https://localhost:8080/","height":468},"execution":{"iopub.status.busy":"2024-05-27T02:15:35.028439Z","iopub.execute_input":"2024-05-27T02:15:35.029004Z","iopub.status.idle":"2024-05-27T02:15:39.378348Z","shell.execute_reply.started":"2024-05-27T02:15:35.028961Z","shell.execute_reply":"2024-05-27T02:15:39.376592Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\nTraining on subject  1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 46\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m train_conf \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m64\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_train\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     43\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearnCurves\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_logits\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mATCNet\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Evaluate the model based on the weights saved in the '/results' folder\u001b[39;00m\n\u001b[1;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m getModel(train_conf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m), dataset_conf)\n","Cell \u001b[0;32mIn[4], line 96\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataset_conf, train_conf, results_path, cwt)\u001b[0m\n\u001b[1;32m     90\u001b[0m             model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39mfrom_logits), optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;66;03m# model.summary()\u001b[39;00m\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;66;03m# plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m             callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 96\u001b[0m                 \u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_weights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     98\u001b[0m                 ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.90\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m),\n\u001b[1;32m     99\u001b[0m                 \u001b[38;5;66;03m# EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m             ]\n\u001b[1;32m    102\u001b[0m             history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_onehot, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val_onehot),\n\u001b[1;32m    103\u001b[0m                                 epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;66;03m# Evaluate the performance of the trained model based on the validation data\u001b[39;00m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;66;03m# Here we load the Trained weights from the file saved in the hard\u001b[39;00m\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;66;03m# disk, which should be the same as the weights of the current model.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#             model.load_weights(filepath)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/callbacks/model_checkpoint.py:183\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_weights_only:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `save_weights_only=True` in `ModelCheckpoint`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, the filepath provided must end in `.weights.h5` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras weights format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mValueError\u001b[0m: When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=/kaggle/working/cognitive-science-final-project/results/saved models_cwt/run-1/subject-1.h5"],"ename":"ValueError","evalue":"When using `save_weights_only=True` in `ModelCheckpoint`, the filepath provided must end in `.weights.h5` (Keras weights format). Received: filepath=/kaggle/working/cognitive-science-final-project/results/saved models_cwt/run-1/subject-1.h5","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"id":"74e020b5"},"execution_count":null,"outputs":[]}]}