{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmq443/cognitive-science-final-project/blob/main/eeg_atcnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "Vj1LMT9OjCpn",
        "outputId": "b972a5b6-d295-470b-e63c-1a0bea9ab910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Vj1LMT9OjCpn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "306e3378-75bf-41b1-ada6-61679182b6dc",
      "metadata": {
        "id": "306e3378-75bf-41b1-ada6-61679182b6dc",
        "outputId": "3977a5ea-dcdf-4e44-f38d-0d4d88e37dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'attention_models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4e33fe6527a4>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mattention_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mattention_block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mattention_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmha_block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocess_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'attention_models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, AveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, SeparableConv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization, LayerNormalization, Flatten\n",
        "from tensorflow.keras.layers import Add, Concatenate, Lambda, Input, Permute\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from attention_models import attention_block\n",
        "from attention_models import mha_block\n",
        "from preprocess_data import *\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489b29c8-e36b-4536-95b2-f909fdd09ae2",
      "metadata": {
        "id": "489b29c8-e36b-4536-95b2-f909fdd09ae2"
      },
      "outputs": [],
      "source": [
        "#%% Convolutional (CV) block used in the ATCNet model\n",
        "def Conv_block(input_layer, F1=4, kernLength=64, poolSize=8, D=2, in_chans=22, dropout=0.1):\n",
        "    \"\"\" Conv_block\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        This block is the same as EEGNet with SeparableConv2D replaced by Conv2D\n",
        "        The original code for this model is available at: https://github.com/vlawhern/arl-eegmodels\n",
        "        See details at https://arxiv.org/abs/1611.08024\n",
        "    \"\"\"\n",
        "    F2= F1*D\n",
        "    block1 = Conv2D(F1, (kernLength, 1), padding = 'same',data_format='channels_last',use_bias = False)(input_layer)\n",
        "    block1 = BatchNormalization(axis = -1)(block1)\n",
        "    block2 = DepthwiseConv2D((1, in_chans), use_bias = False,\n",
        "                                    depth_multiplier = D,\n",
        "                                    data_format='channels_last',\n",
        "                                    depthwise_constraint = max_norm(1.))(block1)\n",
        "    block2 = BatchNormalization(axis = -1)(block2)\n",
        "    block2 = Activation('elu')(block2)\n",
        "    block2 = AveragePooling2D((8,1),data_format='channels_last')(block2)\n",
        "    block2 = Dropout(dropout)(block2)\n",
        "    block3 = Conv2D(F2, (16, 1),\n",
        "                            data_format='channels_last',\n",
        "                            use_bias = False, padding = 'same')(block2)\n",
        "    block3 = BatchNormalization(axis = -1)(block3)\n",
        "    block3 = Activation('elu')(block3)\n",
        "\n",
        "    block3 = AveragePooling2D((poolSize,1),data_format='channels_last')(block3)\n",
        "    block3 = Dropout(dropout)(block3)\n",
        "    return block3\n",
        "\n",
        "def Conv_block_(input_layer, F1=4, kernLength=64, poolSize=8, D=2, in_chans=22,\n",
        "                weightDecay = 0.009, maxNorm = 0.6, dropout=0.25):\n",
        "    \"\"\" Conv_block\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        using  different regularization methods.\n",
        "    \"\"\"\n",
        "\n",
        "    F2= F1*D\n",
        "    block1 = Conv2D(F1, (kernLength, 1), padding = 'same', data_format='channels_last',\n",
        "                    kernel_regularizer=L2(weightDecay),\n",
        "\n",
        "                    # In a Conv2D layer with data_format=\"channels_last\", the weight tensor has shape\n",
        "                    # (rows, cols, input_depth, output_depth), set axis to [0, 1, 2] to constrain\n",
        "                    # the weights of each filter tensor of size (rows, cols, input_depth).\n",
        "                    kernel_constraint = max_norm(maxNorm, axis=[0,1,2]),\n",
        "                    use_bias = False)(input_layer)\n",
        "    block1 = BatchNormalization(axis = -1)(block1)  # bn_axis = -1 if data_format() == 'channels_last' else 1\n",
        "\n",
        "    block2 = DepthwiseConv2D((1, in_chans),\n",
        "                             depth_multiplier = D,\n",
        "                             data_format='channels_last',\n",
        "                             depthwise_regularizer=L2(weightDecay),\n",
        "                             depthwise_constraint  = max_norm(maxNorm, axis=[0,1,2]),\n",
        "                             use_bias = False)(block1)\n",
        "    block2 = BatchNormalization(axis = -1)(block2)\n",
        "    block2 = Activation('elu')(block2)\n",
        "    block2 = AveragePooling2D((8,1),data_format='channels_last')(block2)\n",
        "    block2 = Dropout(dropout)(block2)\n",
        "\n",
        "    block3 = Conv2D(F2, (16, 1),\n",
        "                            data_format='channels_last',\n",
        "                            kernel_regularizer=L2(weightDecay),\n",
        "                            kernel_constraint = max_norm(maxNorm, axis=[0,1,2]),\n",
        "                            use_bias = False, padding = 'same')(block2)\n",
        "    block3 = BatchNormalization(axis = -1)(block3)\n",
        "    block3 = Activation('elu')(block3)\n",
        "\n",
        "    block3 = AveragePooling2D((poolSize,1),data_format='channels_last')(block3)\n",
        "    block3 = Dropout(dropout)(block3)\n",
        "    return block3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b270e0-7d9a-415b-b510-64ee0d7e2fc4",
      "metadata": {
        "id": "c5b270e0-7d9a-415b-b510-64ee0d7e2fc4"
      },
      "outputs": [],
      "source": [
        "#%% Temporal convolutional (TC) block used in the ATCNet model\n",
        "def TCN_block(input_layer,input_dimension,depth,kernel_size,filters,dropout,activation='relu'):\n",
        "    \"\"\" TCN_block from Bai et al 2018\n",
        "        Temporal Convolutional Network (TCN)\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        THe original code available at https://github.com/locuslab/TCN/blob/master/TCN/tcn.py\n",
        "        This implementation has a slight modification from the original code\n",
        "        and it is taken from the code by Ingolfsson et al at https://github.com/iis-eth-zurich/eeg-tcnet\n",
        "        See details at https://arxiv.org/abs/2006.00622\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. Bai, S., Kolter, J. Z., & Koltun, V. (2018).\n",
        "           An empirical evaluation of generic convolutional and recurrent networks\n",
        "           for sequence modeling.\n",
        "           arXiv preprint arXiv:1803.01271.\n",
        "    \"\"\"\n",
        "\n",
        "    block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
        "                   padding = 'causal',kernel_initializer='he_uniform')(input_layer)\n",
        "    block = BatchNormalization()(block)\n",
        "    block = Activation(activation)(block)\n",
        "    block = Dropout(dropout)(block)\n",
        "    block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
        "                   padding = 'causal',kernel_initializer='he_uniform')(block)\n",
        "    block = BatchNormalization()(block)\n",
        "    block = Activation(activation)(block)\n",
        "    block = Dropout(dropout)(block)\n",
        "    if(input_dimension != filters):\n",
        "        conv = Conv1D(filters,kernel_size=1,padding='same')(input_layer)\n",
        "        added = Add()([block,conv])\n",
        "    else:\n",
        "        added = Add()([block,input_layer])\n",
        "    out = Activation(activation)(added)\n",
        "\n",
        "    for i in range(depth-1):\n",
        "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
        "                   padding = 'causal',kernel_initializer='he_uniform')(out)\n",
        "        block = BatchNormalization()(block)\n",
        "        block = Activation(activation)(block)\n",
        "        block = Dropout(dropout)(block)\n",
        "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
        "                   padding = 'causal',kernel_initializer='he_uniform')(block)\n",
        "        block = BatchNormalization()(block)\n",
        "        block = Activation(activation)(block)\n",
        "        block = Dropout(dropout)(block)\n",
        "        added = Add()([block, out])\n",
        "        out = Activation(activation)(added)\n",
        "\n",
        "    return out\n",
        "\n",
        "def TCN_block_(input_layer,input_dimension,depth,kernel_size,filters, dropout,\n",
        "               weightDecay = 0.009, maxNorm = 0.6, activation='relu'):\n",
        "    \"\"\" TCN_block from Bai et al 2018\n",
        "        Temporal Convolutional Network (TCN)\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        using different regularization methods\n",
        "    \"\"\"\n",
        "\n",
        "    block = Conv1D(filters, kernel_size=kernel_size, dilation_rate=1, activation='linear',\n",
        "                    kernel_regularizer=L2(weightDecay),\n",
        "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
        "\n",
        "                    padding = 'causal',kernel_initializer='he_uniform')(input_layer)\n",
        "    block = BatchNormalization()(block)\n",
        "    block = Activation(activation)(block)\n",
        "    block = Dropout(dropout)(block)\n",
        "    block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=1,activation='linear',\n",
        "                    kernel_regularizer=L2(weightDecay),\n",
        "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
        "\n",
        "                    padding = 'causal',kernel_initializer='he_uniform')(block)\n",
        "    block = BatchNormalization()(block)\n",
        "    block = Activation(activation)(block)\n",
        "    block = Dropout(dropout)(block)\n",
        "    if(input_dimension != filters):\n",
        "        conv = Conv1D(filters,kernel_size=1,\n",
        "                    kernel_regularizer=L2(weightDecay),\n",
        "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
        "\n",
        "                    padding='same')(input_layer)\n",
        "        added = Add()([block,conv])\n",
        "    else:\n",
        "        added = Add()([block,input_layer])\n",
        "    out = Activation(activation)(added)\n",
        "\n",
        "    for i in range(depth-1):\n",
        "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
        "                    kernel_regularizer=L2(weightDecay),\n",
        "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
        "\n",
        "                   padding = 'causal',kernel_initializer='he_uniform')(out)\n",
        "        block = BatchNormalization()(block)\n",
        "        block = Activation(activation)(block)\n",
        "        block = Dropout(dropout)(block)\n",
        "        block = Conv1D(filters,kernel_size=kernel_size,dilation_rate=2**(i+1),activation='linear',\n",
        "                    kernel_regularizer=L2(weightDecay),\n",
        "                    kernel_constraint = max_norm(maxNorm, axis=[0,1]),\n",
        "\n",
        "                    padding = 'causal',kernel_initializer='he_uniform')(block)\n",
        "        block = BatchNormalization()(block)\n",
        "        block = Activation(activation)(block)\n",
        "        block = Dropout(dropout)(block)\n",
        "        added = Add()([block, out])\n",
        "        out = Activation(activation)(added)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c159338e-af93-4499-8293-82f85af97bd3",
      "metadata": {
        "id": "c159338e-af93-4499-8293-82f85af97bd3"
      },
      "outputs": [],
      "source": [
        "#%% The proposed ATCNet model, https://doi.org/10.1109/TII.2022.3197419\n",
        "def ATCNet_(n_classes, in_chans = 22, in_samples = 1125, n_windows = 5, attention = 'mha',\n",
        "           eegn_F1 = 16, eegn_D = 2, eegn_kernelSize = 64, eegn_poolSize = 7, eegn_dropout=0.3,\n",
        "           tcn_depth = 2, tcn_kernelSize = 4, tcn_filters = 32, tcn_dropout = 0.3,\n",
        "           tcn_activation = 'elu', fuse = 'average'):\n",
        "\n",
        "    \"\"\" ATCNet model from Altaheri et al 2023.\n",
        "        See details at https://ieeexplore.ieee.org/abstract/document/9852687\n",
        "\n",
        "        Notes\n",
        "        -----\n",
        "        The initial values in this model are based on the values identified by\n",
        "        the authors\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        .. H. Altaheri, G. Muhammad, and M. Alsulaiman. \"Physics-informed\n",
        "           attention temporal convolutional network for EEG-based motor imagery\n",
        "           classification.\" IEEE Transactions on Industrial Informatics,\n",
        "           vol. 19, no. 2, pp. 2249-2258, (2023)\n",
        "           https://doi.org/10.1109/TII.2022.3197419\n",
        "    \"\"\"\n",
        "    input_1 = Input(shape = (1,in_chans, in_samples))   #     TensorShape([None, 1, 22, 1125])\n",
        "    input_2 = Permute((3,2,1))(input_1)\n",
        "\n",
        "    dense_weightDecay = 0.5\n",
        "    conv_weightDecay = 0.009\n",
        "    conv_maxNorm = 0.6\n",
        "    from_logits = False\n",
        "\n",
        "    numFilters = eegn_F1\n",
        "    F2 = numFilters*eegn_D\n",
        "\n",
        "    block1 = Conv_block_(input_layer = input_2, F1 = eegn_F1, D = eegn_D,\n",
        "                        kernLength = eegn_kernelSize, poolSize = eegn_poolSize,\n",
        "                        weightDecay = conv_weightDecay, maxNorm = conv_maxNorm,\n",
        "                        in_chans = in_chans, dropout = eegn_dropout)\n",
        "    block1 = Lambda(lambda x: x[:,:,-1,:])(block1)\n",
        "\n",
        "    # Sliding window\n",
        "    sw_concat = []   # to store concatenated or averaged sliding window outputs\n",
        "    for i in range(n_windows):\n",
        "        st = i\n",
        "        end = block1.shape[1]-n_windows+i+1\n",
        "        block2 = block1[:, st:end, :]\n",
        "\n",
        "        # Attention_model\n",
        "        if attention is not None:\n",
        "            if (attention == 'se' or attention == 'cbam'):\n",
        "                block2 = Permute((2, 1))(block2) # shape=(None, 32, 16)\n",
        "                block2 = attention_block(block2, attention)\n",
        "                block2 = Permute((2, 1))(block2) # shape=(None, 16, 32)\n",
        "            else: block2 = attention_block(block2, attention)\n",
        "\n",
        "        # Temporal convolutional network (TCN)\n",
        "        block3 = TCN_block_(input_layer = block2, input_dimension = F2, depth = tcn_depth,\n",
        "                            kernel_size = tcn_kernelSize, filters = tcn_filters,\n",
        "                            weightDecay = conv_weightDecay, maxNorm = conv_maxNorm,\n",
        "                            dropout = tcn_dropout, activation = tcn_activation)\n",
        "        # Get feature maps of the last sequence\n",
        "        block3 = Lambda(lambda x: x[:,-1,:])(block3)\n",
        "\n",
        "        # Outputs of sliding window: Average_after_dense or concatenate_then_dense\n",
        "        if(fuse == 'average'):\n",
        "            sw_concat.append(Dense(n_classes, kernel_regularizer=L2(dense_weightDecay))(block3))\n",
        "        elif(fuse == 'concat'):\n",
        "            if i == 0:\n",
        "                sw_concat = block3\n",
        "            else:\n",
        "                sw_concat = Concatenate()([sw_concat, block3])\n",
        "\n",
        "    if(fuse == 'average'):\n",
        "        if len(sw_concat) > 1: # more than one window\n",
        "            sw_concat = tf.keras.layers.Average()(sw_concat[:])\n",
        "        else: # one window (# windows = 1)\n",
        "            sw_concat = sw_concat[0]\n",
        "    elif(fuse == 'concat'):\n",
        "        sw_concat = Dense(n_classes, kernel_regularizer=L2(dense_weightDecay))(sw_concat)\n",
        "\n",
        "    if from_logits:  # No activation here because we are using from_logits=True\n",
        "        out = Activation('linear', name = 'linear')(sw_concat)\n",
        "    else:   # Using softmax activation\n",
        "        out = Activation('softmax', name = 'softmax')(sw_concat)\n",
        "\n",
        "    return Model(inputs = input_1, outputs = out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c64240-998d-4d65-9a47-b574ab159566",
      "metadata": {
        "id": "94c64240-998d-4d65-9a47-b574ab159566"
      },
      "outputs": [],
      "source": [
        "def getModel(model_name, dataset_conf, from_logits = False):\n",
        "\n",
        "    n_classes = dataset_conf.get('n_classes')\n",
        "    n_channels = dataset_conf.get('n_channels')\n",
        "    in_samples = dataset_conf.get('in_samples')\n",
        "\n",
        "    # Select the model\n",
        "    if(model_name == 'ATCNet'):\n",
        "        # Train using the proposed ATCNet model: https://ieeexplore.ieee.org/document/9852687\n",
        "        model = ATCNet_(\n",
        "            # Dataset parameters\n",
        "            n_classes = n_classes,\n",
        "            in_chans = n_channels,\n",
        "            in_samples = in_samples,\n",
        "            # Sliding window (SW) parameter\n",
        "            n_windows = 5,\n",
        "            # Attention (AT) block parameter\n",
        "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
        "            # Convolutional (CV) block parameters\n",
        "            eegn_F1 = 16,\n",
        "            eegn_D = 2,\n",
        "            eegn_kernelSize = 64,\n",
        "            eegn_poolSize = 7,\n",
        "            eegn_dropout = 0.3,\n",
        "            # Temporal convolutional (TC) block parameters\n",
        "            tcn_depth = 2,\n",
        "            tcn_kernelSize = 4,\n",
        "            tcn_filters = 32,\n",
        "            tcn_dropout = 0.3,\n",
        "            tcn_activation='elu',\n",
        "            )\n",
        "    '''\n",
        "    elif(model_name == 'TCNet_Fusion'):\n",
        "        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n",
        "        model = models.TCNet_Fusion(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGTCNet'):\n",
        "        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n",
        "        model = models.EEGTCNet(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGNet'):\n",
        "        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n",
        "        model = models.EEGNet_classifier(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGNeX'):\n",
        "        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n",
        "        model = models.EEGNeX_8_32(n_timesteps = in_samples , n_features = n_channels, n_outputs = n_classes)\n",
        "    elif(model_name == 'DeepConvNet'):\n",
        "        # Train using DeepConvNet: https://doi.org/10.1002/hbm.23730\n",
        "        model = models.DeepConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "    elif(model_name == 'ShallowConvNet'):\n",
        "        # Train using ShallowConvNet: https://doi.org/10.1002/hbm.23730\n",
        "        model = models.ShallowConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "    elif(model_name == 'MBEEG_SENet'):\n",
        "        # Train using MBEEG_SENet: https://www.mdpi.com/2075-4418/12/4/995\n",
        "        model = models.MBEEG_SENet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"'{}' model is not supported yet!\".format(model_name))\n",
        "    '''\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6a5b07-c895-48d9-971c-91ef43c6f325",
      "metadata": {
        "id": "9f6a5b07-c895-48d9-971c-91ef43c6f325"
      },
      "outputs": [],
      "source": [
        "def train(dataset_conf, train_conf, results_path):\n",
        "\n",
        "    # remove the 'result' folder before training\n",
        "    if os.path.exists(results_path):\n",
        "        # Remove the folder and its contents\n",
        "        shutil.rmtree(results_path)\n",
        "        os.makedirs(results_path)\n",
        "\n",
        "    # Get the current 'IN' time to calculate the overall training time\n",
        "    in_exp = time.time()\n",
        "    # Create a file to store the path of the best model among several runs\n",
        "    best_models = open(results_path + \"/best models.txt\", \"w\")\n",
        "    # Create a file to store performance during training\n",
        "    log_write = open(results_path + \"/log.txt\", \"w\")\n",
        "\n",
        "    # Get dataset paramters\n",
        "    dataset = dataset_conf.get('name')\n",
        "    n_sub = dataset_conf.get('n_sub')\n",
        "    data_path = dataset_conf.get('data_path')\n",
        "    isStandard = dataset_conf.get('isStandard')\n",
        "    LOSO = dataset_conf.get('LOSO')\n",
        "    # Get training hyperparamters\n",
        "    batch_size = train_conf.get('batch_size')\n",
        "    epochs = train_conf.get('epochs')\n",
        "    patience = train_conf.get('patience')\n",
        "    lr = train_conf.get('lr')\n",
        "    LearnCurves = train_conf.get('LearnCurves') # Plot Learning Curves?\n",
        "    n_train = train_conf.get('n_train')\n",
        "    model_name = train_conf.get('model')\n",
        "    from_logits = train_conf.get('from_logits')\n",
        "\n",
        "    # Initialize variables\n",
        "    acc = np.zeros((n_sub, n_train))\n",
        "    kappa = np.zeros((n_sub, n_train))\n",
        "\n",
        "    # Iteration over subjects\n",
        "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "\n",
        "        print('\\nTraining on subject ', sub+1)\n",
        "        log_write.write( '\\nTraining on subject '+ str(sub+1) +'\\n')\n",
        "        # Initiating variables to save the best subject accuracy among multiple runs.\n",
        "        BestSubjAcc = 0\n",
        "        bestTrainingHistory = []\n",
        "\n",
        "        # Get training and test data\n",
        "        X_train, _, y_train_onehot, _, _, _ = get_data(\n",
        "            data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n",
        "\n",
        "        # Divide the training data into training and validation\n",
        "        X_train, X_val, y_train_onehot, y_val_onehot = train_test_split(X_train, y_train_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Iteration over multiple runs\n",
        "        for train in range(n_train): # How many repetitions of training for subject i.\n",
        "            # Set the random seed for TensorFlow and NumPy random number generator.\n",
        "            # The purpose of setting a seed is to ensure reproducibility in random operations.\n",
        "            tf.random.set_seed(train+1)\n",
        "            np.random.seed(train+1)\n",
        "\n",
        "            # Get the current 'IN' time to calculate the 'run' training time\n",
        "            in_run = time.time()\n",
        "\n",
        "            # Create folders and files to save trained models for all runs\n",
        "            filepath = results_path + '/saved models/run-{}'.format(train+1)\n",
        "            if not os.path.exists(filepath):\n",
        "                os.makedirs(filepath)\n",
        "            filepath = filepath + '/subject-{}.h5'.format(sub+1)\n",
        "\n",
        "            # Create the model\n",
        "            model = getModel(model_name, dataset_conf, from_logits)\n",
        "            # Compile and train the model\n",
        "            model.compile(loss=CategoricalCrossentropy(from_logits=from_logits), optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "            # model.summary()\n",
        "            # plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "            callbacks = [\n",
        "                ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
        "                                save_best_only=True, save_weights_only=True, mode='min'),\n",
        "                ReduceLROnPlateau(monitor=\"val_loss\", factor=0.90, patience=20, verbose=0, min_lr=0.0001),\n",
        "                # EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\n",
        "            ]\n",
        "            history = model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot),\n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)\n",
        "\n",
        "            # Evaluate the performance of the trained model based on the validation data\n",
        "            # Here we load the Trained weights from the file saved in the hard\n",
        "            # disk, which should be the same as the weights of the current model.\n",
        "            model.load_weights(filepath)\n",
        "            y_pred = model.predict(X_val)\n",
        "\n",
        "            if from_logits:\n",
        "                y_pred = tf.nn.softmax(y_pred).numpy().argmax(axis=-1)\n",
        "            else:\n",
        "                y_pred = y_pred.argmax(axis=-1)\n",
        "\n",
        "            labels = y_val_onehot.argmax(axis=-1)\n",
        "            acc[sub, train]  = accuracy_score(labels, y_pred)\n",
        "            kappa[sub, train] = cohen_kappa_score(labels, y_pred)\n",
        "\n",
        "            # Get the current 'OUT' time to calculate the 'run' training time\n",
        "            out_run = time.time()\n",
        "            # Print & write performance measures for each run\n",
        "            info = 'Subject: {}   seed {}   time: {:.1f} m   '.format(sub+1, train+1, ((out_run-in_run)/60))\n",
        "            info = info + 'valid_acc: {:.4f}   valid_loss: {:.3f}'.format(acc[sub, train], min(history.history['val_loss']))\n",
        "            print(info)\n",
        "            log_write.write(info +'\\n')\n",
        "            # If current training run is better than previous runs, save the history.\n",
        "            if(BestSubjAcc < acc[sub, train]):\n",
        "                 BestSubjAcc = acc[sub, train]\n",
        "                 bestTrainingHistory = history\n",
        "\n",
        "        # Store the path of the best model among several runs\n",
        "        best_run = np.argmax(acc[sub,:])\n",
        "        filepath = '/saved models/run-{}/subject-{}.h5'.format(best_run+1, sub+1)+'\\n'\n",
        "        best_models.write(filepath)\n",
        "\n",
        "        # Plot Learning curves\n",
        "        if (LearnCurves == True):\n",
        "            print('Plot Learning Curves ....... ')\n",
        "            draw_learning_curves(bestTrainingHistory, sub+1)\n",
        "\n",
        "    # Get the current 'OUT' time to calculate the overall training time\n",
        "    out_exp = time.time()\n",
        "\n",
        "    # Print & write the validation performance using all seeds\n",
        "    head1 = head2 = '         '\n",
        "    for sub in range(n_sub):\n",
        "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
        "        head2 = head2 + '-----   '\n",
        "    head1 = head1 + '  average'\n",
        "    head2 = head2 + '  -------'\n",
        "    info = '\\n---------------------------------\\nValidation performance (acc %):'\n",
        "    info = info + '\\n---------------------------------\\n' + head1 +'\\n'+ head2\n",
        "    for run in range(n_train):\n",
        "        info = info + '\\nSeed {}:  '.format(run+1)\n",
        "        for sub in range(n_sub):\n",
        "            info = info + '{:.2f}   '.format(acc[sub, run]*100)\n",
        "        info = info + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
        "    info = info + '\\n---------------------------------\\nAverage acc - all seeds: '\n",
        "    info = info + '{:.2f} %\\n\\nTrain Time  - all seeds: {:.1f}'.format(np.average(acc)*100, (out_exp-in_exp)/(60))\n",
        "    info = info + ' min\\n---------------------------------\\n'\n",
        "    print(info)\n",
        "    log_write.write(info+'\\n')\n",
        "\n",
        "    # Close open files\n",
        "    best_models.close()\n",
        "    log_write.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f5f5b1-54b7-4d1f-aee6-33caa8cfd482",
      "metadata": {
        "id": "34f5f5b1-54b7-4d1f-aee6-33caa8cfd482"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "    # Define dataset parameters\n",
        "    dataset = 'BCI2a' # Options: 'BCI2a','HGD', 'CS2R'\n",
        "\n",
        "    # if dataset == 'BCI2a':\n",
        "    in_samples = 1125\n",
        "    n_channels = 22\n",
        "    n_sub = 9\n",
        "    n_classes = 4\n",
        "    classes_labels = ['Left hand', 'Right hand','Foot','Tongue']\n",
        "    # data_path = os.path.expanduser('~') + '/BCI Competition IV/BCI Competition IV-2a/BCI Competition IV 2a mat/'\n",
        "    data_path = './BCICIV-2a-mat'\n",
        "    '''\n",
        "    elif dataset == 'HGD':\n",
        "        in_samples = 1125\n",
        "        n_channels = 44\n",
        "        n_sub = 14\n",
        "        n_classes = 4\n",
        "        classes_labels = ['Right Hand', 'Left Hand','Rest','Feet']\n",
        "        data_path = os.path.expanduser('~') + '/mne_data/MNE-schirrmeister2017-data/robintibor/high-gamma-dataset/raw/master/data/'\n",
        "    elif dataset == 'CS2R':\n",
        "        in_samples = 1125\n",
        "        # in_samples = 576\n",
        "        n_channels = 32\n",
        "        n_sub = 18\n",
        "        n_classes = 3\n",
        "        # classes_labels = ['Fingers', 'Wrist','Elbow','Rest']\n",
        "        classes_labels = ['Fingers', 'Wrist','Elbow']\n",
        "        # classes_labels = ['Fingers', 'Elbow']\n",
        "        data_path = os.path.expanduser('~') + '/CS2R MI EEG dataset/all/EDF - Cleaned - phase one (remove extra runs)/two sessions/'\n",
        "    else:\n",
        "        raise Exception(\"'{}' dataset is not supported yet!\".format(dataset))\n",
        "    '''\n",
        "\n",
        "    # Create a folder to store the results of the experiment\n",
        "    results_path = os.getcwd() + \"/results\"\n",
        "    if not os.path.exists(results_path):\n",
        "      os.makedirs(results_path)   # Create a new directory if it does not exist\n",
        "\n",
        "    # Set dataset paramters\n",
        "    dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n",
        "                    'n_sub': n_sub, 'n_channels': n_channels, 'in_samples': in_samples,\n",
        "                    'data_path': data_path, 'isStandard': True, 'LOSO': False}\n",
        "    # Set training hyperparamters\n",
        "    train_conf = { 'batch_size': 64, 'epochs': 500, 'patience': 100, 'lr': 0.001,'n_train': 1,\n",
        "                  'LearnCurves': True, 'from_logits': False, 'model':'ATCNet'}\n",
        "\n",
        "    # Train the model\n",
        "    train(dataset_conf, train_conf, results_path)\n",
        "\n",
        "    # Evaluate the model based on the weights saved in the '/results' folder\n",
        "    model = getModel(train_conf.get('model'), dataset_conf)\n",
        "    # test(model, dataset_conf, results_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac40e63-15b1-4add-a16a-293d148d4e8e",
      "metadata": {
        "id": "aac40e63-15b1-4add-a16a-293d148d4e8e"
      },
      "outputs": [],
      "source": [
        "#%% Evaluation\n",
        "def test(model, dataset_conf, results_path, allRuns = True):\n",
        "    # Open the  \"Log\" file to write the evaluation results\n",
        "    log_write = open(results_path + \"/log.txt\", \"a\")\n",
        "\n",
        "    # Get dataset paramters\n",
        "    dataset = dataset_conf.get('name')\n",
        "    n_classes = dataset_conf.get('n_classes')\n",
        "    n_sub = dataset_conf.get('n_sub')\n",
        "    data_path = dataset_conf.get('data_path')\n",
        "    isStandard = dataset_conf.get('isStandard')\n",
        "    # LOSO = dataset_conf.get('LOSO')\n",
        "    classes_labels = dataset_conf.get('cl_labels')\n",
        "\n",
        "    # Test the performance based on several runs (seeds)\n",
        "    runs = os.listdir(results_path+\"/saved models\")\n",
        "    # Initialize variables\n",
        "    acc = np.zeros((n_sub, len(runs)))\n",
        "    kappa = np.zeros((n_sub, len(runs)))\n",
        "    cf_matrix = np.zeros([n_sub, len(runs), n_classes, n_classes])\n",
        "\n",
        "    # Iteration over subjects\n",
        "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "    inference_time = 0 #  inference_time: classification time for one trial\n",
        "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "        # Load data\n",
        "        _, _, _, X_test, _, y_test_onehot = get_data(\n",
        "            path=data_path,\n",
        "            subject=sub,\n",
        "            is_shuffle=True,\n",
        "            is_standard = isStandard\n",
        "        )\n",
        "\n",
        "        # Iteration over runs (seeds)\n",
        "        for seed in range(len(runs)):\n",
        "            # Load the model of the seed.\n",
        "            model.load_weights('{}/saved models/{}/subject-{}.h5'.format(results_path, runs[seed], sub+1))\n",
        "\n",
        "            inference_time = time.time()\n",
        "            # Predict MI task\n",
        "            y_pred = model.predict(X_test).argmax(axis=-1)\n",
        "            inference_time = (time.time() - inference_time)/X_test.shape[0]\n",
        "            # Calculate accuracy and K-score\n",
        "            labels = y_test_onehot.argmax(axis=-1)\n",
        "            acc[sub, seed]  = accuracy_score(labels, y_pred)\n",
        "            kappa[sub, seed] = cohen_kappa_score(labels, y_pred)\n",
        "            # Calculate and draw confusion matrix\n",
        "            cf_matrix[sub, seed, :, :] = confusion_matrix(labels, y_pred, normalize='true')\n",
        "            # draw_confusion_matrix(cf_matrix[sub, seed, :, :], str(sub+1), results_path, classes_labels)\n",
        "\n",
        "    # Print & write the average performance measures for all subjects\n",
        "    head1 = head2 = '                  '\n",
        "    for sub in range(n_sub):\n",
        "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
        "        head2 = head2 + '-----   '\n",
        "    head1 = head1 + '  average'\n",
        "    head2 = head2 + '  -------'\n",
        "    info = '\\n' + head1 +'\\n'+ head2\n",
        "    info = '\\n---------------------------------\\nTest performance (acc & k-score):\\n'\n",
        "    info = info + '---------------------------------\\n' + head1 +'\\n'+ head2\n",
        "    for run in range(len(runs)):\n",
        "        info = info + '\\nSeed {}: '.format(run+1)\n",
        "        info_acc = '(acc %)   '\n",
        "        info_k = '        (k-sco)   '\n",
        "        for sub in range(n_sub):\n",
        "            info_acc = info_acc + '{:.2f}   '.format(acc[sub, run]*100)\n",
        "            info_k = info_k + '{:.3f}   '.format(kappa[sub, run])\n",
        "        info_acc = info_acc + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
        "        info_k = info_k + '  {:.3f}   '.format(np.average(kappa[:, run]))\n",
        "        info = info + info_acc + '\\n' + info_k\n",
        "    info = info + '\\n----------------------------------\\nAverage - all seeds (acc %): '\n",
        "    info = info + '{:.2f}\\n                    (k-sco): '.format(np.average(acc)*100)\n",
        "    info = info + '{:.3f}\\n\\nInference time: {:.2f}'.format(np.average(kappa), inference_time * 1000)\n",
        "    info = info + ' ms per trial\\n----------------------------------\\n'\n",
        "    print(info)\n",
        "    log_write.write(info+'\\n')\n",
        "\n",
        "    # Draw a performance bar chart for all subjects\n",
        "    draw_performance_barChart(n_sub, acc.mean(1), 'Accuracy')\n",
        "    draw_performance_barChart(n_sub, kappa.mean(1), 'k-score')\n",
        "    # Draw confusion matrix for all subjects (average)\n",
        "    draw_confusion_matrix(cf_matrix.mean((0,1)), 'All', results_path, classes_labels)\n",
        "    # Close opened file\n",
        "    log_write.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8745c989-94a2-43ae-ae46-fa113b32f050",
      "metadata": {
        "id": "8745c989-94a2-43ae-ae46-fa113b32f050"
      },
      "outputs": [],
      "source": [
        "def draw_performance_barChart(num_sub, metric, label):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = list(range(1, num_sub+1))\n",
        "    ax.bar(x, metric, 0.5, label=label)\n",
        "    ax.set_ylabel(label)\n",
        "    ax.set_xlabel(\"Subject\")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_title('Model '+ label + ' per subject')\n",
        "    ax.set_ylim([0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676148fd-f48b-4411-8502-ea8e0679e810",
      "metadata": {
        "id": "676148fd-f48b-4411-8502-ea8e0679e810"
      },
      "outputs": [],
      "source": [
        "def draw_confusion_matrix(cf_matrix, sub, results_path, classes_labels):\n",
        "    # Generate confusion matrix plot\n",
        "    display_labels = classes_labels\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix,\n",
        "                                display_labels=display_labels)\n",
        "    disp.plot()\n",
        "    disp.ax_.set_xticklabels(display_labels, rotation=12)\n",
        "    plt.title('Confusion Matrix of Subject: ' + sub )\n",
        "    plt.savefig(results_path + '/subject_' + sub + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e1201a-8aba-4e56-adfd-1c61361971d9",
      "metadata": {
        "id": "40e1201a-8aba-4e56-adfd-1c61361971d9"
      },
      "outputs": [],
      "source": [
        "def draw_learning_curves(history, sub):\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy - subject: ' + str(sub))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss - subject: ' + str(sub))\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14bbb8bc-b59f-44bc-965b-d028473c2c83",
      "metadata": {
        "id": "14bbb8bc-b59f-44bc-965b-d028473c2c83"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4af82d4-4fb5-4799-a01d-f8afd074143c",
      "metadata": {
        "id": "f4af82d4-4fb5-4799-a01d-f8afd074143c",
        "outputId": "26d5c153-410d-47a5-af66-381403de60c6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03melif dataset == 'HGD': \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    in_samples = 1125\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    raise Exception(\"'{}' dataset is not supported yet!\".format(dataset))\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Create a folder to store the results of the experiment\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m results_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(results_path):\n\u001b[1;32m     38\u001b[0m   os\u001b[38;5;241m.\u001b[39mmakedirs(results_path)   \u001b[38;5;66;03m# Create a new directory if it does not exist \u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a420508-5f94-40eb-89f2-d932a3e99cb3",
      "metadata": {
        "id": "7a420508-5f94-40eb-89f2-d932a3e99cb3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}